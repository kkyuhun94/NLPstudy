{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c74265",
   "metadata": {},
   "source": [
    "### fugashi\n",
    "* 일본어 tokenizer\n",
    "* ref : fugashi, a Tool for Tokenizing Japanese in Python(EMNLP,2020)\n",
    "    * https://www.aclweb.org/anthology/2020.nlposs-1.7/\n",
    "* 맥북으로 돌려서 100만개 문장을 돌리다 에러가 자주나서 10만개씩 끊어서 토큰화함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d0b3185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'麩 菓子 は 、 麩 を 主材 料 とした 日本 の 菓子 。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fugashi import GenericTagger\n",
    "tagger = GenericTagger()\n",
    "text = \"麩菓子は、麩を主材料とした日本の菓子。\"\n",
    "\n",
    "# # parse can be used as normal\n",
    "# tagger.parse('something')\n",
    "\n",
    "# features from the dictionary can be accessed by field numbers\n",
    "' '.join([str(word) for word in tagger(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5f09b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'麩 菓子 は 、 麩 を 主材 料 とした 日本 の 菓子 。'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_sentence(\"麩菓子は、麩を主材料とした日本の菓子。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "909ce6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_tokenizer(input_path : str, output_path : str) :\n",
    "    \"\"\"\n",
    "    input : input file path\n",
    "    output : .txt file (consist of joined tokens)\n",
    "    tokenize with tokenize sentence \n",
    "    \"\"\"\n",
    "    # read file\n",
    "    txt_file = open(input_path, encoding='utf-8').read().split('\\n')\n",
    "    # tokenize per line\n",
    "    joined_token_list = [tokenize_sentence(line) for line in txt_file[:100000]]\n",
    "    tokenize_text = \"\\n\".join(joined_token_list)\n",
    "    \n",
    "    # save a text file\n",
    "    f = open(output_path ,'w')\n",
    "    f.write(tokenize_text)\n",
    "    f.close()\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8766b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 14.4 s, total: 27.8 s\n",
      "Wall time: 27.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='koja_ja_100000.txt' mode='w' encoding='UTF-8'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "file_tokenizer('JPC4.3/traindev/ko-ja/train.ja', 'koja_ja_100000.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ab1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_file = open('JPC4.3/traindev/ko-ja/train.ja', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cee7c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ja_file[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d8038bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ja_file[100000:200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08d04abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C量が0.5%未満では、機械構造部品や工具の素材として必要な強度が得られない。',\n",
       " 'そのため、Nの含有量は0.01%以下に限定する。',\n",
       " 'それぞれの範囲の下限未満の含有量では、添加の効果が十分に得られない。',\n",
       " '結果を表3に示す。']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_file[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bc44c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C量が0.5%未満では、機械構造部品や工具の素材として必要な強度が得られない。', 'そのため、Nの含有量は0.01%以下に限定する。']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_file[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3eaa2b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['それぞれの範囲の下限未満の含有量では、添加の効果が十分に得られない。', '結果を表3に示す。']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_file[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7239ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugashi import GenericTagger\n",
    "\n",
    "def tokenize_sentence(sentence : str) -> str :\n",
    "    \"\"\"\n",
    "    input : txt sentence\n",
    "    ouput : joined token with whitespace\n",
    "    1.tokenize\n",
    "    2.join the token\n",
    "    \"\"\"\n",
    "    tagger = GenericTagger()\n",
    "    joined_token = ' '.join([str(word) for word in tagger(sentence)])\n",
    "    \n",
    "    return joined_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f9151b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def file_tokenizer(input_path : str, output_path : str) :\n",
    "    \"\"\"\n",
    "    input : input file path\n",
    "    output : .txt file (consist of joined tokens)\n",
    "    tokenize with tokenize sentence \n",
    "    \"\"\"\n",
    "    # read file\n",
    "    txt_file = open(input_path, encoding='utf-8').read().split('\\n')\n",
    "    # tokenize per line\n",
    "    joined_token_list = [tokenize_sentence(line) for line in txt_file[900000:]]\n",
    "    tokenize_text = \"\\n\".join(joined_token_list)\n",
    "    \n",
    "    # save a text file\n",
    "    f = open(output_path ,'w')\n",
    "    f.write(tokenize_text)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9948fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 s, sys: 14.7 s, total: 28.6 s\n",
      "Wall time: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_tokenizer('JPC4.3/traindev/ko-ja/train.ja', 'koja_ja_100000_10.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c127d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
